# Traitements avanc√©s dans un loader

## Qu‚Äôest-ce qu‚Äôun loader dans Lodex ?

Dans **Lodex**, un loader est un fichier de configuration permettant d'importer un jeu de donn√©es dans une instance.  

Son r√¥le est de convertir **un fichier brut** (CSV, JSON, XML, etc.) en un flux d‚Äôobjets **JavaScript** pr√™ts √† √™tre stock√©s dans **la base MongoDB de Lodex**.  

```mermaid
graph TD;
  A["Fichier brut (CSV, JSON, XML...)"] --> B["Loader (script EZS)"];
  B --> C["Objets JavaScript"];
  C --> D["Base MongoDB (Lodex)"];
```

Un loader s'√©crit dans un **fichier `.ini`**.  
- `.ini` = *Initialization file* (fichier d‚Äôinitialisation).  
- Historiquement, ce format sert √† d√©crire des configurations sous forme texte, avec des **sections** (`[ ]`) et des paires `cl√© = valeur`.  
- Lodex r√©utilise ce format pour d√©crire un **pipeline de transformation**.  

Un fichier `.ini` suit une logique d'√©tapes : il est lu de haut en bas et est ex√©cut√© s√©quentiellement.  
Chaque section √©tant une √©tape du pipeline qui applique une transformation au flux de donn√©es avant de le transmettre √† la suivante.  

üëâ Exemple :  

```ini
[assign]
path = Unpaywall
value = get("Identifiers.DOI")

[swing]
test = get("Unpaywall").isEmpty()
reverse = true

[swing/expand]
path = Unpaywall
size = 100

[swing/expand/URLConnect]
url = https://biblio-tools.services.istex.fr/v2/unpaywall/works/expand
timeout = 3600000
noerror = false
retries = 5
```

üîç Explications :

```txt
        [assign]    ‚Üí cr√©ation du champ "Unpaywall" √† partir du champ "DOI"
           ‚îÇ
           ‚ñº
        [swing]     ‚Üí On teste si le champ est vide (absence de DOI), si c'est le cas on exclue ces donn√©es du traitement
           ‚îÇ
           ‚ñº
     [swing/expand]     ‚Üí On regroupe les donn√©es par paquet de 100
           ‚îÇ
           ‚ñº
 [swing/expand/URLConnect]    ‚Üí On interroge l‚ÄôAPI Unpaywall
           ‚îÇ
           ‚ñº
 On r√©cup√®re les information d'Unpaywall dans notre champ
```

Ce mod√®le en **pipeline** rend les loaders tr√®s **flexibles** : on peut ajouter, retirer ou modifier des √©tapes sans casser l‚Äôensemble.  

---

Ecrire ses transformations dans un *loader* plut√¥t que dans **Lodex** en *enrichissements* pr√©sente plusieurs avantages :  

- Tout d'abord, **Lodex** fonctionne en **stream** :
  - Lorsqu‚Äôon applique des enrichissements, Lodex travaille par paquets.  
  - Chaque paquet est transform√©, envoy√© dans MongoDB, puis relu pour passer √† l‚Äô√©tape suivante.
  - Cela provoque de nombreux allers-retours entre Lodex et MongoDB, ce qui rallonge les temps de traitement.
- Un loader agit diff√©remment :
  - Le fichier est lu et transform√© √©tape par √©tape dans le pipeline EZS.  
  - Toutes les transformations sont effectu√©es **avant** l‚Äô√©criture en base.
  - Puis le flux final d‚Äôobjets JavaScript est envoy√© en une seule fois √† MongoDB.
- Et surtout, un loader permet de r√©aliser des op√©rations impossibles ou limit√©es dans Lodex:
  - **Lodex** traite chaque "notice" ind√©pendamment (*ou ligne par ligne)*.
  - Un loader peut lire et transformer tout le dataset √† la fois, ce qui rend possibles des op√©rations globales comme :
    - d√©doublonner des lignes enti√®res (*notices identiques*),
    - fusionner des donn√©es,
    - ou appliquer des transformations lourdes sur l‚Äôensemble.  

üìå **En r√©sum√©, Lodex traite les donn√©es ligne par ligne et nous pousse √† raisonner notice par notice, tandis qu‚Äôun loader permet de r√©fl√©chir en termes d‚Äôop√©rations globales sur l‚Äôensemble du dataset, ce qui en augmente consid√©rablement le potentiel de transformation.**

---

Pour qu'un loader fonctionne correctement, il faut inclure des instructions **EZS** sp√©cifiques (comme `[unpack]`, `[identify]`, `[OBJFlatten]`‚Ä¶) et d√©clarer des *plugins*, mais ce n'est pas l'objet de cette documentation.  

Ces aspects techniques rel√®vent davantage d‚Äôune formation d√©di√©e √† **EZS**, √ßa tombe bien ! Mon coll√®gue Fran√ßois Parmentier en a justement fait une [ici](https://github.com/parmentf/formation-ezs/tree/master).  

Nous allons nous concentrer ici sur un ensemble restreint d‚Äôinstructions essentielles pour transformer et nettoyer les donn√©es avec Lodash.  

## Les instructions EZS

### [assign]

`[assign]` permet d'affecter une valeur √† un champ de l'objet courant. Si le champ existe d√©j√†, sa valeur est √©cras√©e, sinon il est cr√©√©.

```json
[{
    "DO": "10.3390/info10050178 ",
    "TI": "Istex: A Database of Twenty Million Scientific Papers with a Mining Tool Which Uses Named Entities",
    "SO": "Information"
}]
```

```js
[assign]
path = doi
value = get("DO")
```

:point_down:

```json
[{
    "DO": "10.3390/info10050178 ",
    "TI": "Istex: A Database of Twenty Million Scientific Papers with a Mining Tool Which Uses Named Entities",
    "SO": "Information",
    "doi": "10.3390/info10050178 "
}]
```

Dans un `[assign]`, on peut cr√©er autant de nouveaux champs que l‚Äôon veut, tant qu‚Äôils s‚Äôappuient uniquement sur les champs existants du dataset original.  

On peut donc les d√©finir dans le m√™me bloc `[assign]`.

```js
[assign]
path = doi
value = get("DO")

path = normalizedTitle
value = get("TI").deburr().toLower()

path = normalizedSource
value = get("SO").deburr().toLower()
```

‚ö†Ô∏è Attention :  

Un champ cr√©√© dans un `[assign]` n‚Äôest pas utilisable imm√©diatement dans ce m√™me bloc.  
Il ne devient disponible qu‚Äô√† la sortie du bloc, c‚Äôest-√†-dire pour les instructions suivantes.  

Si l'on souhaite cr√©er un champ contenant le doi ou, s'il n'existe pas, le titre normalis√©, on ne peut ajouter ceci dans le bloc :

```js
path = doiOrNormalizedTitle
value = get("doi").thru(doi => _.isEmpty(doi) ? self.normalizedTitle : doi)
```

Il faut ouvrir un nouvel `[assign]`

```js
[assign]
path = doiOrNormalizedTitle
value = get("doi").thru(doi => _.isEmpty(doi) ? self.normalizedTitle : doi)
```

R√©sultat :

```json
[{
    "DO": "10.3390/info10050178 ",
    "TI": "Istex: A Database of Twenty Million Scientific Papers with a Mining Tool Which Uses Named Entities",
    "SO": "Information",
    "doi": "10.3390/info10050178 ",
    "normalizedTitle": "istex: a database of twenty million scientific papers with a mining tool which uses named entities",
    "normalizedSource": "information",
    "doiOrNormalizedTitle": "10.3390/info10050178 "
}]
```

üîë **√Ä retenir : `[assign]` permet d‚Äôajouter de nouveaux champs ou de modifier des champs existants (s‚Äôils portent le m√™me nom), tout en conservant l‚Äôobjet original.**  
**On enrichit donc ce dernier, au lieu de le remplacer.**

---

### [replace]

On a vu que l‚Äôon pouvait modifier des valeurs ou ajouter de nouveaux champs avec `[assign]`, ce qui permet d‚Äôenrichir l‚Äôobjet tout en conservant ses donn√©es originales.  

Mais il arrive qu‚Äôon trouve les noms de champs du dataset peu intelligibles (DO, TI, SO...) ou pas adapt√©s √† nos besoins. Plut√¥t que d'empiler des `[assign]` puis de supprimmer les champs originaux ensuite, il faut utiliser `[replace]` qui permet de reconstruire un objet en lieu et place de l'original.  

Ainsi : 

```js
[replace]
path = doi
value = get("DO")

path = normalizedTitle
value = get("TI").deburr().toLower()

path = normalizedSource
value = get("SO").deburr().toLower()
```

retourne :

```json
[{
    "doi": "10.3390/info10050178 ",
    "normalizedTitle": "istex: a database of twenty million scientific papers with a mining tool which uses named entities",
    "normalizedSource": "information"
}]
```

L‚Äôinstruction `[replace]` permet de **remplacer compl√®tement l‚Äôobjet courant** par un nouveau que l'on d√©finit dans le m√™me bloc.  

Si l‚Äôon veut cr√©er un champ suppl√©mentaire √† partir de l‚Äôobjet d√©j√† modifi√©, comme dans l'exemple d'`[assign]` avec *doiOrNormalizedTitle* attention au pi√®ge !

Il faudra ici ajouter `[assign]` pour cr√©er le champ *doiOrNormalizedTitle* et non pas `[replace]`.  

Si l'on ouvre un nouveau bloc `[replace]`, il va remplacer celui que l'on avait cr√©√© juste avant :

```js
[replace]
path = doi
value = get("DO")

path = normalizedTitle
value = get("TI").deburr().toLower()

path = normalizedSource
value = get("SO").deburr().toLower()

[replace]
path = doiOrNormalizedTitle
value = get("doi").thru(doi => _.isEmpty(doi) ? self.normalizedTitle : doi)
```

:point_down:

```json
[{
    "doiOrNormalizedTitle": "10.3390/info10050178 "
}]
```

`[replace]` n'est donc pas cumulatif comme l'est `[assign]`.

üîë **√Ä retenir : [replace] reconstruit un objet neuf en rempla√ßant compl√®tement l‚Äôobjet courant. Tous les champs non explicitement red√©finis disparaissent.**

---

### [exchange]

`[exchange]` est souvent difficile √† diff√©rencier de `[assign]` ou `[replace]`, on pourrait dire qu'il se situe un peu entre les deux.  
Lui aussi remplace l'objet entier, mais le contenu du nouvel objet est transform√© √† partir de l'objet courant.  

Un exemple simple :

```json
[
  { "nom": "un", "valeur": 1 },
  { "nom": "deux", "valeur": 2 }
]
```

```js
[exchange]
value = get("nom")
```

Sortie :  

```js
["un","deux"]
```

Ici, chaque objet est remplac√© par sa valeur du champ *nom*.  

Combin√© √† **Lodash** `[exchange]` peut avoir d'autres utilit√©s, comme supprimer des cl√©s d'un objet, ce qui en d'autres termes signifie **retirer des colonnes ind√©sirables de son dataset.**

```json
[
  { "nom": "un", "valeur": 1 },
  { "nom": "deux", "valeur": 2 }
]
```

```js
[exchange]
value = omit("valeur")
```

Sortie :  

```js
[
  { "nom": "un" },
  { "nom": "deux" }
]
```

> [!TIP]
> Pour supprimer plusieurs colonnes ou champs en m√™me temps, on met simplement la liste des champs in√©sirables dans un tableau.
> ```js
> [exchange]
> value = omit(["colonneX","colonneY","colonneZ"])
> ```

Plus puissant encore, l√† ou `[assign]` permettait de d'ajouter ou de cr√©er des champs un par un, `[exchange]` permet d'agir sur **l'objet entier** et donc de r√©aliser des **transformations globales**.  

Un cas d'usage pour illustrer, o√π l'on souhaitait r√©aliser la m√™me transformation sur 10 colonnes du dataset. A savoir, verbaliser des bool√©ens en remplacant *true* par *disponible* et *false* par *non disponible*.

```json
[
  {
    "publication_title": "Cancer Research",
    "publication_type": "serial",
    "IN2P3": true,
    "INC": false,
    "INEE": true,
    "INP": false,
    "INS2I": true,
    "INSB": false,
    "INSHS": true,
    "INSIS": false,
    "INSMI": true,
    "INSU": false,
    "type_acces": "acc√®s acquis"
  }
]
```

```js
[exchange]
value = self().mapValues((value, key) => \
    ['IN2P3', 'INC', 'INEE', 'INP', 'INS2I', 'INSB', 'INSHS', 'INSIS', 'INSMI', 'INSU'].includes(key) \
        ? value === true ? 'disponible' : 'non disponible' \
        : value \
)
```

:point_down:

```json
[
  {
    "publication_title": "Cancer Research",
    "publication_type": "serial",
    "IN2P3": "disponible",
    "INC": "non disponible",
    "INEE": "disponible",
    "INP": "non disponible",
    "INS2I": "disponible",
    "INSB": "non disponible",
    "INSHS": "disponible",
    "INSIS": "non disponible",
    "INSMI": "disponible",
    "INSU": "non disponible",
    "type_acces": "acc√®s acquis"
  }
]
```

On a donc conserv√© notre objet complet : toutes les cl√©s s√©lectionn√©es ont vu leurs valeurs transform√©es et les autres cl√©s sont rest√©es inchang√©es.  

D√©componsons le script :

- `self()` permet de r√©cup√©rer l'objet dans son int√©gralit√©.
- `.mapValues((value, key) => ‚Ä¶ )` permet de parcourir toutes les cl√©s de l'objet et d'appliquer une fonction √† chaque paire cl√©/valeur par `=>`.
- `.includes(key)` est une **condition logique** qui v√©rifi√© si la cl√© en cours appartient √† la liste (des instituts)
  - si la cl√© appartient √† la liste alors on applique √† la valeur la transformation `value === true ? 'disponible' : 'non disponible'`
  - ou si la cl√© n'appartient pas √† la liste, on laisse la valeur telle quelle par `: value`.

Vous l'aurez compris, combiner l'instruction **EZS** `[exchange]` avec des fonctions **Lodash** ouvre la voie √† des **transformations globales, nombreuses et vari√©es** que nous ne d√©taillerons pas plus ici mais dans les scripts avanc√©s et cas d'usage.  

---
 
### [remove]

`[remove]` permet de supprimer ent√®rement certains √©l√©ments du flux en fonction d‚Äôune condition. Les objets qui ne satisfont pas le test sont √©cart√©s et ne passent pas √† l‚Äô√©tape suivante.  

Autrement dit, dans le contexte de **Lodex** cela permet de supprimer des **lignes enti√®res**.

```json
[
  { "id": 1, "type": "article" },
  { "id": 2, "type": "book chapter" },
  { "id": 3, "type": "preprint" },
  { "id": 4, "type": "article" }
]
```

```js
[remove]
test = get("type").isEqual("preprint")
```

On souhaite ici supprimer les objets dont le `type` est "preprint".

```json
[{
    "id": 1,
    "type": "article"
},
{
    "id": 2,
    "type": "book chapter"
},
{
    "id": 4,
    "type": "article"
}]
```

Si l'on souhaitait conserver uniquement le `type` "article", soit supprimer tout sauf "article", il suffit d'inverser le test.

```js
[remove]
test = get("type").isEqual("article")
reverse = true
```

```json
[{
    "id": 1,
    "type": "article"
},
{
    "id": 4,
    "type": "article"
}]
```

---


### [dedupe]

Supprime les objets qui sont **consid√©r√©s comme identiques**.  

Par d√©faut `[dedupe]` agit sur le champ `uri`.  
Cela signifie qu'il ne supprime pas automatiquement des objets identiques en tout point, il faut toujours r√©fl√©chir au bon crit√®re de d√©doublonnage.  

Ici on veut d√©doublonner des notices bibliographiques en se basant sur le DOI :

```json
[
  { "doi": "10.123/abc", "title": "A" },
  { "doi": "10.456/def", "title": "B" },
  { "doi": "10.123/abc", "title": "A" }
]
```

```js
[dedupe]
path = doi
ignore = true
```

```json
[{
    "doi": "10.123/abc",
    "title": "A"
},
{
    "doi": "10.456/def",
    "title": "B"
}]
```

‚ö†Ô∏è Il est imp√©ratif de mettre `ignore = true`. Par d√©faut le param√®tre `ignore` est `false`, il stoppe alors le flux de traitements d√®s qu'un doublon est rencontr√© :  
`item #3 [dedupe] <Error: Duplicate identifier: 10.123/abc already exists>`  
`ignore = true` permet de ne garder que le 1er √©l√©ment parmi ceux qui sont identiques et de poursuivre le flux de traitements.  

Si le dataset n‚Äôa pas d‚Äôidentifiant unique fiable, on peut vouloir comparer l‚Äôint√©gralit√© de l‚Äôobjet pour d√©tecter les doublons.  
Cette approche est plus complexe (parce qu‚Äôon doit comparer toutes les cl√©s et valeurs de l‚Äôobjet), et sera donc pr√©sent√©e dans le chapitre suivant.  

---

### [exploding]

L‚Äôinstruction `[exploding]` permet de d√©multiplier les √©l√©ments d‚Äôun tableau contenu dans un champ en plusieurs lignes.  
Chaque valeur contenue dans le tableau devient alors une ligne ind√©pendante.  

Elle prend deux param√®tres :

- `id` : le champ qui sert ¬´ d‚Äôidentifiant ¬ª et reste inchang√©
- `value` : le champ qui contient un tableau et qui sera ¬´ explos√© ¬ª


```json
[
  {
    "title": "Exploring OpenAlex",
    "keywords": ["OpenAlex", "bibliom√©trie", "CNRS"]
  }
]
```

Dans cet exemple on veut obtenir autant de lignes qu'il y a de mots-cl√©s, tout en gardant le lien entre un mot cl√© et le titre de la publication dont il est issu.  

```js
[exploding]
id = title
value = keywords
```

:point_down:

```json
[{
    "id": "Exploring OpenAlex",
    "value": "OpenAlex"
},
{
    "id": "Exploring OpenAlex",
    "value": "bibliom√©trie"
},
{
    "id": "Exploring OpenAlex",
    "value": "CNRS"
}]
```

Cela fonctionne de cette fa√ßon pour un jeux de donn√©es compos√© de deux champs seulement. S'il y en a plus, et c'est souvent le cas, il faut restructurer l'objet avant et apr√®s `[exploding]`.

```json
[
  {
    "title": "Exploring OpenAlex",
    "doi": "10.212121",
    "keywords": ["OpenAlex", "bibliom√©trie", "CNRS"]
  }
]
```

```js
[replace]
path = id
value = self().omit("keywords")
// pour conserver tous les autres champs on les regroupe dans id. On enl√®ve keywords pour ne pas l'avoir en double.

path = value
value = get("keywords")
// On assigne keywords √† value pour qu'il soit "explos√©".

[exploding]
id = id
value = value
```

Ici notre dataset a bien √©t√© d√©multipli√© en autant de "keywords" qu'il y en a mais on r√©cup√®re un objet avec seulement deux champs `id` et `value`.

```json
[{
    "id": {
      "title": "Exploring OpenAlex",
        "doi": "10.212121"
    },
    "value": "OpenAlex"
},
{
  "id": {
    "title": "Exploring OpenAlex",
        "doi": "10.212121"
    },
    "value": "bibliom√©trie"
},
{
  "id": {
    "title": "Exploring OpenAlex",
        "doi": "10.212121"
    },
    "value": "CNRS"
}]
```
De fa√ßon plus visuelle : 

| id (objet regroup√©) | value |
|---------------------|-------|
| `{ "title": "Exploring OpenAlex", "doi": "10.212121" }` | OpenAlex |
| `{ "title": "Exploring OpenAlex", "doi": "10.212121" }` | bibliom√©trie |
| `{ "title": "Exploring OpenAlex", "doi": "10.212121" }` | CNRS |

Il faudrait donc pouvoir "d√©baller" tous les champs contenus dans `id` afin d'en faire des colonnes dans **Lodex**.  
Pour cela on utilisera l'instruction `[exchange]` avec un peu de Lodash.

```js
[exchange]
value = self() \
    .assign({keyword: self.value}) \
    .assign(self.id) \
    .omit(["id", "value"])  

```

- `assign({ keyword: self.value })` ‚Üí on cr√©e une nouvelle cl√© `keyword` avec le contenu de `value`
- `assign(self.id)` ‚Üí on ajoute les champs contenus dans `id` (ici `title` et `doi`)

A ce stade, notre dataset se compose d√©sormais de 5 cl√©s : `id`, `value`, `keyword`, `title` et `doi`. Ne reste plus qu'√† supprimer `id` et `value`.
 
- `omit(["id", "value"])`  

On obtient bien un "dataset" de 3 objets (3 lignes), compos√©s de 3 cl√©s (3 colonnes).  

:point_down:

```json
[{
    "keyword": "OpenAlex",
    "title": "Exploring OpenAlex",
    "doi": "10.212121"
},
{
    "keyword": "bibliom√©trie",
    "title": "Exploring OpenAlex",
    "doi": "10.212121"
},
{
    "keyword": "CNRS",
    "title": "Exploring OpenAlex",
    "doi": "10.212121"
}]
```

---

### [aggregate]

Cette instruction permet d'agr√©ger des objets, autrement dit de fusionner des lignes d'apr√®s un crit√®re pr√©cis.  

Manipuler `[aggregate]` s'av√®re plus complexe que les autres instructions **EZS** vues pr√©c√©demment.  

Allons-y pas √† pas : 

```json
[
  { "doi": "10.1/abc", "dataBaseId": "WOS", "title": "bla bla bla" },
  { "doi": "10.1/abc", "dataBaseId": "OpenAlex", "title": "bla bla bla" },
  { "doi": "10.2/def", "dataBaseId": "HAL", "title": "bla bla" }
]
```

On souhaite ici fusionner les lignes ayant un *doi* en commun, tout en conservant toutes les informations propres aux autres champs.  

> [!WARNING] 
> `[aggregate]` ne fonctionne qu'avec une paire `id`/`value` o√π `id` sera le crit√®re d'agr√©gation et `value` les donn√©es agr√©g√©es.  

Il faut donc commencer par transformer notre dataset en une paire `id`/`value` :

```js
[replace]
path = id
value = get("doi")

path = value
value = self()
```

On remplace donc notre dataset original compos√© de 3 cl√©s en assignant la valeur du champ `doi` √† `id` et en regroupant l'ensemble du dataset(`self`) dans `value`.

```json
[{
  "id": "10.1/abc",
    "value": {
      "doi": "10.1/abc",
        "dataBaseId": "WOS",
        "title": "bla bla bla"
    }
},
{
  "id": "10.1/abc",
    "value": {
      "doi": "10.1/abc",
        "dataBaseId": "OpenAlex",
        "title": "bla bla bla"
    }
},
{
  "id": "10.2/def",
    "value": {
      "doi": "10.2/def",
        "dataBaseId": "HAL",
        "title": "bla bla"
    }
}]
```

On peut maintenant faire notre agr√©gation en ajoutant : 

```js
[aggregate]
path = id
```

L'agr√©gation fonctionne bien mais l'objet final demanderait √† √™tre restructur√©. Si chaque identifiant id est bien unique, la cl√© value contient simplement un tableau des objets regroup√©s.

```json
[{
  "id": "10.1/abc",
    "value": [
      {
        "doi": "10.1/abc",
            "dataBaseId": "WOS",
            "title": "bla bla bla"
        },
        {
          "doi": "10.1/abc",
            "dataBaseId": "OpenAlex",
            "title": "bla bla bla"
        }
    ]
},
{
  "id": "10.2/def",
    "value": [
      {
        "doi": "10.2/def",
            "dataBaseId": "HAL",
            "title": "bla bla"
        }
    ]
}]
```

`[aggregate]` ne fusionne pas les champs, il les juxtapose.  

Pour "fusionner" les lignes il faut donc ajouter quelques instructions pour restructurer le tableau : 

```js
[replace]

; Tous les champs √©tant dans un tableau value, il convient de mapper toutes les cl√©s

path = doi
value = get("value").map("doi").first()
; S'il y a plusieurs doi, ils seront forc√©ment identiques, on peut donc forcer une cha√Æne avec first

path = dataBaseId
value = get("value").map("dataBaseId")

path = title
value = get("value").map("title")
```

On r√©cup√®re bien un r√©sultat final conforme et bien structur√© : 

```json
[{
  "doi": "10.1/abc",
    "dataBaseId": [
      "WOS",
        "OpenAlex"
    ],
    "title": [
      "bla bla bla",
        "bla bla bla"
    ]
},
{
  "doi": "10.2/def",
    "dataBaseId": [
      "HAL"
    ],
    "title": [
      "bla bla"
    ]
}]
```

> [!TIP]
> On peut ajouter des transformations afin de savoir ce qui a √©t√© agr√©g√© et en quelle quantit√©.

```js
path = isAggregated
value = get("value").size().thru(numb=>numb===1 ? "Non agr√©g√©" : "Agr√©g√©")

path = worksNumber
value = get("value").size()
```

Ces 2 champs nous informeront sur quelles lignes ont √©t√© agr√©g√©es ou non, ainsi que le nombre de lignes agr√©g√©es en une seule.  

```json
[{
  "doi": "10.1/abc",
    "dataBaseId": [
      "WOS",
        "OpenAlex"
    ],
    "title": [
      "bla bla bla",
        "bla bla bla"
    ],
    "isAggregated": "Agr√©g√©",
    "worksNumber": 2
}]
```

> [!WARNING] 
> Si une ligne n‚Äôa pas de valeur pour le champ qui sert d'`id` (ici le doi), alors elle est purement et simplement supprim√©e du flux !!!

Pour √©viter cela on doit g√©n√©rer une valeur de repli (fallback), mais qui doit n√©cessairement √™tre unique. Sinon tous les objets sans doi seront agr√©g√©s en un seul.  

On utilisera donc ceci :  
`_.now().toString(36) + Math.random().toString(36).substr(2)`  

On renvoie la date en millisecondes, que l'on convertit en base 36, on y ajoute un nombre flottant entre 0 et 1, on convertit de nouveau en base 36 puis on enl√®ve "0.". Cela donne des `id` de ce type :  

```json
[
{    "id": "mfu5iwtr933w2udkp7q"},
{    "id": "mfu5iwtry8lsbtri68"},
{    "id": "mfu5iwtstvs86nojdmj"},
{    "id": "mfu5iwtsa6dx1m8clh7"}
]
```

On peut placer ces fonctions directement sur le champ `id` ce qui donne un script final comme ceci :  

```js
[replace]
path = id
value = get("doi").thru(doi =>doi === ""? _.now().toString(36) + Math.random().toString(36).substr(2): doi)

path = value
value = self()

[aggregate]
path = id

[replace]

path = doi
value = get("value").map("doi").first()

path = dataBaseId
value = get("value").map("dataBaseId")

path = title
value = get("value").map("title")

path = isAggregated
value = get("value").size().thru(numb=>numb===1 ? "Non agr√©g√©" : "Agr√©g√©")

path = worksNumber
value = get("value").size()
```

> [!TIP]
> On verra dans les [Scripts avanc√©s et cas d'usage](https://github.com/AnaelKremer/Atelier-Lodash-usage-Lodex/blob/main/11-cas-dusage.md) comment combiner les instructions `[exploding]` et `[aggregate]`.

---

### [env] D√©finir des fonctions r√©utilisables

On a d√©j√† vu qu'avec l'instruction `[ENV]` il √©tait possible de stocker un dictionnaire que l'on pouvait r√©utiliser.  

De la m√™me mani√®re, `[ENV]` peut contenir des **fonctions utilitaires**, que l'on d√©finit une fois et que l'on r√©utilise ensuite √† plusieurs endroits du pipeline.  

Par exemple, on a un jeu de donn√©es qui va contenir beaucoup de *bool√©ens* (is openacces, has repository, is retracted etc). Dans Lodex on souhaiterait afficher des "Oui" ou "Non" plut√¥t que les *bool√©ens*.  

On va d'abord √©cire notre fonction dans `[ENV]` et la nommer correctement (c'est souvent la partie la plus compliqu√©e).

```js
[env]
path = labelizeBoolean
value = fix((bool)=> bool === true ? "Oui" : bool === false ? "Non" : "Inconnu")
```

On utilise `fix` pour d√©clarer notre fonction, puis on √©crit une simple *ternaire*.  

Il suffit ensuite dans un `[assign]` de d√©clarer le champ que l'on veut transformer, puis d'appeler la *variable d'environnement* dans un `thru`.

```js
[assign]
path = value
value = get("isRetracted").thru(env("labelizeBoolean"))
// Entr√©e : true ‚Üí Sortie : "Oui"
```

Et si notre champ ne contient pas un seul, mais plusieurs *bool√©ens* dans un tableau, il suffit de remplacer `thru` par `map` !

```js
[assign]
path = value
value = get("isCnrs").map(env("labelizeBoolean"))
// Entr√©e :  [false, true, true, false] ‚Üí Sortie : ["Non","Oui","Oui","Non"]
```

Une fois cette logique acquise, on peut aller encore plus loin et se lancer dans **la composition de fonctions**.  

Plut√¥t que d‚Äô√©crire de longs traitements redondants, on cr√©e de petites briques sp√©cialis√©es (nettoyer une cha√Æne, nettoyer un tableau), puis on les encha√Æne et les combine pour construire des transformations plus complexes.  

---

A titre d'exemple, on souhaite "normaliser" les champs contenant des cha√Ænes de caract√®res (titre, r√©sum√©...). On souhaite √©galement r√©aliser ces op√©rations sur les √©l√©ments de certains tableaux comme les mots cl√©s. Et enfin on souhaite que donn√©es sous formes de tableaux soient d√©doublonn√©es, tri√©es et vid√©es de leurs valeurs falsy (null, ""...).  

On construit notre premi√®re brique : 

```js
[env]
path = normalizeString
value = fix((str)=> \
  _.toLower( \
    _.deburr( \
      _.trim(str) \
    ) \
  ) \
)
```

> [!NOTE]
> On le sait, utiliser `=>` nous fait passer dans du **JavaScript pur**.  
> Mais pour r√©aliser un cha√Ænage **Lodash** dans une fonction anonyme il convient de mettre le pr√©fixe `_` avant chaque fonction comme ici.  
> Mais lorsque l'on combine des fonctions, la lecture peut devenir difficile. Dans cet exemple, `trim` est d'abord utilis√©e, puis `deburr` et enfin `toLower`.
>
> Il existe une autre fa√ßon de d√©clarer un cha√Ænage, qui est sans doute plus lisible :
>
> Il faut placer la valeur √† transformer dans un **wrapper Lodash**. Litt√©ralement cela signifie que l'on *emballe* la valeur dans un objet sp√©cial afin de pouvoir la passer dans un pipeline de transformations.  
> Et comme vu [ici](https://github.com/AnaelKremer/Atelier-Lodash-usage-Lodex/blob/main/01-introduction.md#un-encha%C3%AEnement-de-fonctions-lodash) un pipeline **Lodash** doit commencer par `_.chain` et se conclure par `.value()`.

On peut donc √©crire notre premi√®re brique comme cela :

```js
[env]
path = normalizeString
value = fix((str)=> \
  _.chain(str) \
    .trim() \
    .deburr() \
    .toLower() \
    .value() \
)
```

Puis la tester sur une cha√Æne de caract√®res :

```js
[assign]
path = normalizedTitle
value = get("value.title").thru(env("normalizeString"))
// Entr√©e :  "Bibliom√©trie pr√™te √† l'emploi avec OpenAlex : retour d'exp√©rience" 
// ‚Üí Sortie : "bibliometrie prete a l'emploi avec openalex : retour d'experience"
```

On peut ensuite cr√©er notre deuxi√®me brique destin√©e √† traiter des tableaux (que l'on ajoute dans [ENV] en dessous de la premi√®re) :

```js
path = cleanArray
value = fix((arr) => \
  _.chain(arr) \
    .compact() \
    .uniq() \
    .sort() \
    .value() \
)
```

On peut enfin utiliser nos deux briques sur un m√™me champ :

```js
path = normalizedAllKeywords
value = get("value.authorsKeywords") \
    .concat(self.value.keywords) \
    .map(env("normalizeString")) \
    .thru(env("cleanArray"))
```

:point_down:

```json
{
  "authorsKeywords": [
    "bibliom√©trie",
    "openalex",
    "cnrs-inist",
    "retour d‚Äôexp√©rience"
  ],
  "keywords": [
    "bibliometrie",
    "OpenAlex",
    "CNRS-INIST",
    "exp√©rience utilisateur",
    "Lodex"
  ],
  "normalizedAllKeywords": [
    "bibliometrie",
    "cnrs-inist",
    "experience utilisateur",
    "lodex",
    "openalex",
    "retour d‚Äôexperience"
  ]
}
```